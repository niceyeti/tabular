What concurrency patterns exist to coordinate n goroutine updates to a large matrix of m entries? i.e., m >> n.
The general problem is when the number of critical resources (matrix entries) significantly exceeds
the number of workers (e.g. NumCPU). Thus contention is
sparse, but locking around every contended item is wasteful or infeasible. No ordering of
access to matrix entries is assumed; workers access entries randomly.
The matrix size is fixed at runtime and the entries are structs.

For background, this is for parallelizing Monte Carlo and RL methods in an app for personal golang review.
Monte Carlo methods are specifically useful in huge state spaces, for which m >> n,
thus parallelizing over n workers is beneficial.
My bandaid solution uses atomic ops: cast *float64 to *uint in order to leverage sync/atomic operations for all
reads/writes. This works okay, for now.
But I'm curious about solving the more general case of a large, fixed data structure (for which the sync/atomic
approach is not possible).

Facts to keep in mind:
* multi-dimensional matrices are just a representation of linear data; any tuple of matrix indices is merely an aliasing
scheme for a single-index linear array of data.
* traditional C-programming approaches patterns are a subset of solutions in Go, including lowest level
  strategies like using CompareAndSwap instructions


Getting into the weeds, the critical values are really the n numbers (matrix position) of each
worker; thus the problem reduces to ensuring the uniqueness of n integers. Log(m) locks
is possible if one hashed the matrix indices to a smaller set of locks. Is there instead a 
CSP-style pattern for communicating instead of locking?
Some spitball solutions:
1) Implement matrix entries as AtomicFloat64: this likely provides the most general solution for concurrent numerical ops
2) Hash item indices to a smaller set of locks. This is similar to the concept of ShardedMap in 
'Cloud Native Go' but is fairly fundamental: maintain a map of locks, indexed using a space-reduction scheme
of some kind. For instance, workers could lock on row indices, thus generally they would be separated in some manner.
But a more advanced scheme would use more locks, such as modding the row and column indices by a radius value 'r', and thus
workers would lock only if on the same 'tile' (a sub region of the matrix) of size `r x r`. However for this hashing scheme
the number of locks is still a function of m.
3) Refactored workers: workers generate and send individual matrix updates over a chan to a single writer
Domain-specific solutions for RL:
4) Location manager: workers derive their transitions from a single policy routine/oracle that ensures they traverse to unique positions.
   But this provides only incidental coordination.
5) Workers TryWriteLock() around a bit-set or sync.map of current worker positions; if a state transition would collide with another worker,\
   pick a different state at random until TryWriteLock() succeeds.
   This could be done using a sync.Map of agent current locations.
Likely best solutions to generic problem of n-workers locking m items, where n << m:
6) AtomicBitset: Implement an atomic bit-set, declare one of size i*j,
 and then have agents call Take(i,j) to set the bit at that location. It either:
 - succeeds, indicating the agent has taken (i,j) (it must also unconditionally release its old (i',j')!)
 - fails due to CompareAndSwap returning false
 - fails because the bit is already set, indicating another worker occupies (i,j)
 At any rate, the agent can then do something else: try another location, wait, etc.
    Pro/con:
    + Solves the problem using a compressed representation
    - Still O(m), though in a compressed space.
    - Non-locking and non-CSP. How to know memory-model requirements are met? Such as cpu's flushing their memory to (i,j) entries, and similar consistency issues.


A motivating thought experiment: a tree of channels? This is throwaway, the intent
is to divulge the problem's components. Perhaps there is an underlying data structure
for this problem.
    P-queue of semaphores: the root node is a channel (semaphore) of size N; then bifurcate
    downward to n/2 nodes, n/4 nodes... until 0. The tree size is then ~N*Lg(N). Workers
    could query this tree for properties of interest, for example 'would-lock' queries would
    allow them to query another potential position when a state space became crowded by
    adjacent workers. This could, on average, be used to encourage workers to separate from
    eachother in the state space, leveraging the bifurcation properties of the tree (e.g.
    "only allow n/2 workers per left/right side of state space").
    - seems fraught, deadlock prone
    - the tree structure only applies to 2D state spaces, not graphical ones for instance
    - still O(m), and may more or less be an approximation of a sync.Map whose keys are
      matrix indices
    * see: http://alumni.cs.ucr.edu/~ysong/cs160/lab9/lock-chaining.html

Problem statement: query and add set items to a disjoint set of numbers of size n.
    Data structures:
    - binary trees: unclear how these help
    - hashmap: perfectly solves problem, but uses coarse locking
    - disjoint-set: these are for partitioning populations of items, no singletons
    - number theory: for small n (e.g., 8), can't this work? use bit ops to preserve the uniqueness of only 8 numbers.
      For example, XOR can determine uniqueness: 1^2^3^2^1 = 3. But it does not work if a number occurs thrice.
    - bitsets?

Number theory spitballing:
    Alternating compare-and-swap and i+j concat:
        For small n, leverage CompareAndSwap: if n <= 8, and i/j are modest, compare-and-swap concat(i,j)
        with portions of eight portions of a machine word; if any match, then those indices are in use.
        For a word size of 64 bits, i and j indices allows for 64/16 = 4 bits per i and j. Thus matrix
        size limit is 16 x 16.
    XOR attempt:
        XOR can find dupes (but only of size 2) in number sequences: 1^2^1 = 2.
            Query: does set {1,3,5} contain 5?
                1 ^ 3 ^ 5 ^ 5 = 1 ^ 3
    Conclusion: the sequential construction could be done, but is not a good idea. This construction uses the
    property of XOR to determine if (i,j) is occupied for worker n_i. A big drawback is that there is no guarantee
    of a worker disassociating itself with a position after using it, as would be the case with mutexes (defer mu.Unlock()).
    An AtomicBitset satisfies the same logical requirements, with fewer bitwise hijinks.

Required properties:
- a depth of O(n*log(n)) for data of size O(m)
- every path to a leaf uniquely encode something in the space of O(m), e.g. matrix indices

N-ary Tree spitballing:
    Take(i,j):
        // Find node_i_j from parity of i and j, just like a tradition binary tree
        // If entry (i,j) exists, then it is in use
            - retry for another (i',j')
        // Else: create node (i,j) and release previous (i",j")
        if i % 2 == 0:
            return Take(i/2, j)
        return Take(i, j/2)

    N-ary tree where n is numworkers. Leverage semaphores to determine the number of locks held
    below some subtree, representing a region of indices.

    (i1,j1) (i2,j2) (i3, j3)
    Query: is (i_k, j_l) taken?

        Bifurcate the number of indices (could also be b-tree, e.g. mod 3, mod 4, etc.):

                            root
                      ________|________
                     /                 \ 
                    / i even            \ i odd
           j even  /\ j odd      j even /\ j odd

        And place a locked hashmap at each node, whose keys are concat(i,j).
        By maintaining separate hashmaps, the likelihood of collision decreases.
        Topology still means up to n entries must be stored per node.

Brutest force: maintain an array of size i*j. Index into it using concat(i,j)
and check if entry is 1 (in-use). This is the same as storing such fields in
each matrix entry, mem complexity is O(i*j)==O(m). But can this be reduced to O(i+j)?
An array of size i, whose entries are j's.
    Query: InUse(i,j,arr) {
        arr[i] == j
    }
    Note this lock per-row (where i is a row number). This could be improved by ensuring
    that the entries represent sets of j's instead of a single j. Using bit ops, on a 64-bit
    architecture, could each entry represent up to n (numworkers) entries? For n=8, 64/8 = 8,
    for up to 256 bits. Thus a 256x256 matrix could be represented.
    But can we efficiently query and store these sub-values in entries using CompareAndSwap?
    I'm assuming so, but am uncertain; specifically: does arr[i] contain j's bit pattern
    in its component bits?
    * If all (i,j) are unique, then arr[i] components will never XOR to zero (except when all zero).
        bi ^ b0 ^ b1 ^ b2 ^ b3 ^ b4... != 0

        Using CompareAndSwap:
        Take: set arr[i] to arr[i] ^ j, only if arr[i] ^ j != 0.
        Give: set arr[i] to arr[i] ^ j, only if arr[i] ^ j == 0.

Useful data structures: k-d trees (k==2) are amenable to collision detecting and nearest-neighbor range searches,
though such a tree would have to be updated based on the dynamically-changing coordinates of agents. This
could be done a number of ways. But a google search for 'k-d trees collision detection' will yield some of the
research and other relevant data structures, borrowed from the field of game programming (much of which is the
same problem).



In many textbook examples like the classic 2d 'gridworld', matrix indices represent discrete states and matrix entries are state-value estimates.
'Agents' (as n goroutines) navigate the matrix
and update the state values per visit or per episode, potentially stomping eachother's updates.
Most of these algorithms can be guaranteed to converge despite such race conditions, but they would fail
golang's '-race' build flag and allowing races feels fraught ("there is no such thing as a benign data race").

Some spitball solutions:
- Fine-grain locking (brute force): lock around every matrix entry.
- Atomic updating: implement atomic float updates by casting them to uints

I feel like I'm missing a more general approach, such as using a concurrent datastructure like a sync.Map.
The number of locks seems like it should be tied to n, not m, for the general problem of synchronizing
n goroutines' access to m resources; this is is what interests me about the problem.

FWIW my first throwaway pass ignores races: start n agents and one 'estimator'. The agents generate episodes (matrix
transition sequences) and send these via a chan to the estimator, which solely updates the matrix values.



Assume you have a very large matrix with millions of entries and n workers, where n is small (e.g. ~NumCpu).
Readers read a value from the matrix and transition to new location, until they reach a terminal location, generating a sequence of location+value pairs.
Writers receive these sequences (via chan) and use them to update the values at each location in the matrix.
But the gist is: you have n agents at random locations in the matrix (and they only read values),
while elsewhere you have an updater sweeping through and updating the (visited) values of the matrix.




- Brute-force fine-grain locking: lock every entry in the matrix
- Atomicize operations: one could redefine the problem in integer space and thereby use the sync/atomic
- Put all entries into a sync.Pool



Writer:
  - claim entry (i,j), update its value, release (i,j)
Reader:
  - is (i,j) free?
    * yes: read it
    * no: wait for it

Observe: matrix indices probably map into a much smaller space of bits; for example, if the indices are ints, just concat them.
Thus checking if a state is in use could be an ~O(1) operation.



Data structure approach:
    - Use bitmap? A bitvector may support up to 64 * 64
        Writer:
        - 

    - Use sync.Map?
        Writer:
            * Put (i,j) in map, update it, remove it when done
        Reader:
            * Check if (i,j) in map:
                - yes: wait for deletion
                - no: continue/use it
    - Use sync.Pool?
        Writer:
            * put (i,j) into sync.Pool
        Reader:
            * check for (i',j') in sync.Pool


